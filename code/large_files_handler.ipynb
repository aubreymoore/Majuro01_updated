{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lined-interference",
   "metadata": {},
   "source": [
    "# large_files_handler.ipynb\n",
    "GitHub repositories require files to be less than 50 MB.\n",
    "However, large files can be split into smaller ones, stored in GitHub, and reassembled after cloning.\n",
    "\n",
    "In this notebook, I use a three Linux commands, **find**, **split** and **cat**, to implement this strategy.\n",
    "\n",
    "The notebook adds paths for large files in **.gitignore** to prevent them being added to the repo.\n",
    "\n",
    "**find** is used to list large files. Note that '..' is included to start the search in the parent folder, which is the top level folder for the repository. In this example, the commanline returns three file paths.\n",
    "```\n",
    "find .. -type f -size +50M'\n",
    "\n",
    "../code/object-detectors/inference_data/frozen_inference_graph_5classes.pb\n",
    "../code/object-detectors/inference_data/frozen_inference_graph_3classes.pb\n",
    "../code/object-detectors/inference_data/mask_rcnn_cvat_0160.h5\n",
    "```\n",
    "\n",
    "**split** is used to split the files in 40 MB parts which can be stored in a GitHub repo.\n",
    "\n",
    "```\n",
    "split -b 40MB {filepath} {filepath}.part_\n",
    "```\n",
    "\n",
    "**cat** is used to reassemble parts into the original file.\n",
    "```\n",
    "cat {filepath}.part_?? > {filepath}\n",
    "```\n",
    "\n",
    "My idea is to run this notebook at the beginning of a workflow to handle large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cutting-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "south-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_large_files_list():\n",
    "    \"\"\" \n",
    "    Returns a list of filenames for files greater than 50 Mb \n",
    "    Note that recursive search starts in parent folder.\n",
    "    \"\"\"\n",
    "    result = subprocess.run('find .. -type f -size +50M', stdout=subprocess.PIPE, shell=True)\n",
    "    return [i.decode('utf8') for i in result.stdout.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "final-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_large_files_are_listed_in_gitignore(large_files_list):\n",
    "\n",
    "    # if ../.gitignore does not exist, create it\n",
    "    gitignore_path = '../.gitignore'\n",
    "    if not os.path.exists(gitignore_path):\n",
    "        os.system(f'touch {gitignore_path}')\n",
    "\n",
    "    # read ../.gitignore as a string\n",
    "    with open(gitignore_path, 'r') as f:\n",
    "        s = f.read()          \n",
    "\n",
    "    # make sure that a filename for each file greater than 50 Mb is included in the string\n",
    "    string_has_been_modified = False\n",
    "    for filename in large_files_list:\n",
    "        found = any(filename in x for x in s)\n",
    "        if not found:\n",
    "            s = s + filename + '\\n'\n",
    "            string_has_been_modified = True\n",
    "\n",
    "    # if the string has been modified, replace ../.gitignore\n",
    "    if string_has_been_modified:\n",
    "        with open(gitignore_path, 'w') as f:\n",
    "            f.write(s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fuzzy-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_large_files_from_parts():\n",
    "    \"\"\" each large file is assembeled only if it does not already exist \"\"\"\n",
    "    parts_files = sorted(glob.glob('../**/*.part_*', recursive=True))\n",
    "    for file in list(set([s[:s.find('.part_')] for s in parts_files])):\n",
    "        if not os.path.exists(file):\n",
    "            command = f'cat {file}.part_?? > {file}'\n",
    "            print(command)\n",
    "            subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brown-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_large_files(large_file_list):\n",
    "    \"\"\" each large file is split only if parts do not already exist \"\"\"\n",
    "    for filename in large_file_list:\n",
    "        parts = glob.glob(f'{filename}.part_*', recursive=True)\n",
    "        if len(parts) == 0:\n",
    "            command = f'split -b 40MB {filename} {filename}.part_'\n",
    "            print(command)\n",
    "            subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protective-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following large files (>50 Mb) are included in .gitignore\n",
      "Parts lists for these files facilitate storage in GitHub repos.\n",
      "Running ths notebook (large_file_handler.ipynb) will re-assemble large files from the parts files.\n",
      "../code/object-detectors/inference_data/frozen_inference_graph_5classes.pb\n",
      "../code/object-detectors/inference_data/frozen_inference_graph_3classes.pb\n",
      "../code/object-detectors/inference_data/mask_rcnn_cvat_0160.h5\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "large_files_list = get_large_files_list()\n",
    "if len(large_files_list) == 0:   # this is the state immediately after cloning from github\n",
    "    assemble_large_files_from_parts()\n",
    "else:\n",
    "    split_large_files(large_files_list)\n",
    "    \n",
    "large_files_list = get_large_files_list()\n",
    "confirm_large_files_are_listed_in_gitignore(large_files_list)\n",
    "print('The following large files (>50 Mb) are included in .gitignore')\n",
    "print('Parts lists for these files facilitate storage in GitHub repos.')\n",
    "print('Running ths notebook (large_file_handler.ipynb) will re-assemble large files from the parts files.')\n",
    "for f in large_files_list:\n",
    "    print(f)\n",
    "print('FINISHED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
